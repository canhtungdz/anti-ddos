from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, trim, upper
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
import os

# ğŸš€ Táº¡o Spark session
spark = SparkSession.builder \
    .appName("Train Binary Random Forest for DDoS") \
    .getOrCreate()
spark.sparkContext.setLogLevel("WARN")

# ğŸ“¥ Äá»c táº¥t cáº£ file .csv
df = spark.read.csv("/opt/spark-data/*.csv", header=True, inferSchema=True)

# âœ‚ï¸ LÃ m sáº¡ch tÃªn cá»™t
df = df.toDF(*[c.strip() for c in df.columns])
for old_name in df.columns:
    new_name = old_name.replace(" ", "_").replace(".", "_")
    if old_name != new_name:
        df = df.withColumnRenamed(old_name, new_name)

# ğŸ§¹ XoÃ¡ cá»™t khÃ´ng cáº§n thiáº¿t
cols_to_drop = ['Unnamed:_0', 'Flow_ID', 'Source_IP', 'Destination_IP', 'Timestamp', 'SimillarHTTP', 'Inbound']
df = df.drop(*[col_name for col_name in cols_to_drop if col_name in df.columns])

# ğŸ”¤ LÃ m sáº¡ch nhÃ£n (Label)
df = df.withColumn("Label_cleaned", upper(trim(col("Label"))))

# ğŸ” Chuyá»ƒn nhÃ£n sang binary: BENIGN â†’ 0.0, cÃ²n láº¡i â†’ 1.0
df = df.withColumn("binary_label", when(col("Label_cleaned") == "BENIGN", 0.0).otherwise(1.0))

# ğŸ§½ Thay tháº¿ Inf vÃ  NaN
for c in df.columns:
    if c not in ["binary_label", "Label", "Label_cleaned"]:
        df = df.withColumn(c, when(col(c).isin(float("inf"), float("-inf")), None).otherwise(col(c)))

df = df.dropna()

# ğŸ›‘ Kiá»ƒm tra rá»—ng
if df.count() == 0:
    raise ValueError("âŒ Dá»¯ liá»‡u rá»—ng sau khi lÃ m sáº¡ch!")

# ğŸ“Š Thá»‘ng kÃª phÃ¢n phá»‘i nhÃ£n
print("ğŸ“Š PhÃ¢n phá»‘i nhÃ£n sau xá»­ lÃ½:")
df.groupBy("binary_label").count().orderBy("binary_label").show()

# ğŸ§® Vector hÃ³a Ä‘áº·c trÆ°ng
feature_cols = [c for c in df.columns if c not in ["Label", "Label_cleaned", "binary_label"]]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# ğŸŒ² Random Forest phÃ¢n loáº¡i nhá»‹ phÃ¢n
rf = RandomForestClassifier(
    labelCol="binary_label",
    featuresCol="features",
    numTrees=100,
    maxDepth=6,
    impurity="gini",
    featureSubsetStrategy="sqrt"
)

# ğŸ”— Pipeline
pipeline = Pipeline(stages=[assembler, rf])

# âœ‚ï¸ Chia train/test
train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)

# ğŸ§  Huáº¥n luyá»‡n
print("ğŸš€ Äang huáº¥n luyá»‡n mÃ´ hÃ¬nh Random Forest cÃ³ cÃ¢n báº±ng nhÃ£n...")
rf_model = pipeline.fit(train_data)

# ğŸ’¾ LÆ°u mÃ´ hÃ¬nh
model_path = "/opt/ml-model/rf_binary_model"
rf_model.write().overwrite().save(model_path)
print(f"âœ… MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u táº¡i: {model_path}")

# âœï¸ Ghi danh sÃ¡ch Ä‘áº·c trÆ°ng ra file
features_txt_path = "/opt/ml-model/expected_features.txt"
os.makedirs("/opt/ml-model", exist_ok=True)
with open(features_txt_path, "w") as f:
    for col_name in feature_cols:
        f.write(col_name + "\n")
print(f"âœ… ÄÃ£ lÆ°u danh sÃ¡ch Ä‘áº·c trÆ°ng vÃ o: {features_txt_path}")

# ğŸ” Dá»± Ä‘oÃ¡n
predictions = rf_model.transform(test_data)

# ğŸ§¾ Ma tráº­n nháº§m láº«n
print("\nğŸ“‰ Ma tráº­n nháº§m láº«n:")
predictions.groupBy("binary_label", "prediction").count().orderBy("binary_label", "prediction").show()

# ğŸ¯ ÄÃ¡nh giÃ¡
evaluator = MulticlassClassificationEvaluator(labelCol="binary_label", predictionCol="prediction")
accuracy = evaluator.setMetricName("accuracy").evaluate(predictions)
precision = evaluator.setMetricName("weightedPrecision").evaluate(predictions)
recall = evaluator.setMetricName("weightedRecall").evaluate(predictions)
f1 = evaluator.setMetricName("f1").evaluate(predictions)

# ğŸ“ˆ In káº¿t quáº£
print("\nğŸ“ˆ Káº¾T QUáº¢ ÄÃNH GIÃ MÃ” HÃŒNH:")
print(f"ğŸ¯ Accuracy        : {accuracy * 100:.2f}%")
print(f"ğŸ“Œ Precision       : {precision * 100:.2f}%")
print(f"ğŸ“¥ Recall          : {recall * 100:.2f}%")
print(f"ğŸ† F1 Score        : {f1 * 100:.2f}%")
