# DDoS Detection using Spark MLlib (Random Forest Model)

## üìò M·ª•c ti√™u

·ª®ng d·ª•ng ph√°t hi·ªán c√°c lu·ªìng d·ªØ li·ªáu DDoS theo th·ªùi gian th·ª±c (real-time) b·∫±ng c√°ch s·ª≠ d·ª•ng m√¥ h√¨nh Random Forest ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n Spark MLlib. D·ªØ li·ªáu ƒë·∫ßu v√†o ƒë·∫øn t·ª´ Kafka v√† ƒë·∫ßu ra l√† nh√£n ph√¢n lo·∫°i: `Normal` ho·∫∑c `DDoS`.

---

## Y√™u c·∫ßu h·ªá th·ªëng

* Apache Spark 3.x (h·ªó tr·ª£ Structured Streaming)
* Kafka topic ch·ª©a d·ªØ li·ªáu real-time (v√≠ d·ª•: `ddos_packets_raw`)
* M√¥ h√¨nh hu·∫•n luy·ªán s·∫µn (`rf_binary_model`)
* File ƒë·∫∑c tr∆∞ng y√™u c·∫ßu (`expected_features.txt`)
* Docker Compose (n·∫øu ch·∫°y theo d·∫°ng c·ª•m Spark + Kafka)

---

## C·∫•u tr√∫c th∆∞ m·ª•c

```
spark_app/
‚îú‚îÄ‚îÄ ml_model/
‚îÇ   ‚îú‚îÄ‚îÄ rf_binary_model/               #  M√¥ h√¨nh Random Forest ƒë√£ hu·∫•n luy·ªán
‚îÇ   ‚îú‚îÄ‚îÄ expected_features.txt          #  Danh s√°ch c√°c c·ªôt ƒë·∫∑c tr∆∞ng
‚îÇ   ‚îî‚îÄ‚îÄ README.md                      #  T√†i li·ªáu m√¥ t·∫£ m√¥ h√¨nh
‚îú‚îÄ‚îÄ predict_rf_spark.py               #  D·ª± ƒëo√°n batch tr√™n file CSV
‚îú‚îÄ‚îÄ train_rf.py                       # Script hu·∫•n luy·ªán m√¥ h√¨nh Random Forest
‚îú‚îÄ‚îÄ main.py                           # (tu·ª≥ ch·ªçn) ch·∫°y ch√≠nh
‚îî‚îÄ‚îÄ README.md                         #  H∆∞·ªõng d·∫´n t·ªïng th·ªÉ
```

---

## C√°ch hu·∫•n luy·ªán m√¥ h√¨nh (1 l·∫ßn duy nh·∫•t)

> N·∫øu b·∫°n ch∆∞a c√≥ m√¥ h√¨nh, h√£y ch·∫°y script sau ƒë·ªÉ train:

```bash
docker exec -it spark-master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-apps/train_rf.py
```

Sau khi ch·∫°y xong, th∆∞ m·ª•c `/opt/ml-model/rf_binary_model` s·∫Ω ch·ª©a pipeline ƒë√£ hu·∫•n luy·ªán. File `/opt/ml-model/expected_features.txt` c≈©ng ƒë∆∞·ª£c t·∫°o k√®m.

---

## C√°ch ch·∫°y d·ª± ƒëo√°n real-time

### 1. Kafka ph·∫£i g·ª≠i d·ªØ li·ªáu l√™n topic (v√≠ d·ª• `ddos_packets_raw`), m·ªói b·∫£n ghi l√† 1 JSON object v·ªõi c√°c ƒë·∫∑c tr∆∞ng ƒë·ªãnh l∆∞·ª£ng (numeric features).

V√≠ d·ª• m·ªôt b·∫£n ghi:

```json
{
  "Unnamed_0": 1,
  "Source_IP": "192.168.1.1",
  "Destination_IP": "10.0.0.5",
  "Timestamp": "2025-07-10T15:00:00Z",
  "SimillarHTTP": 0,
  "Inbound": 1,
  "Flow_Duration": 12000,
  "Total_Fwd_Packets": 10,
  "Total_Backward_Packets": 5
  // ... c√°c ƒë·∫∑c tr∆∞ng kh√°c
}
```

> ‚ö†Ô∏è B·∫°n c·∫ßn x·ª≠ l√Ω ƒë·∫ßu v√†o ƒë·ªÉ lo·∫°i b·ªè c√°c c·ªôt kh√¥ng n·∫±m trong danh s√°ch `expected_features.txt`, ch·∫≥ng h·∫°n nh∆∞: `Unnamed_0`, `Source_IP`, `Destination_IP`, `Timestamp`, `SimillarHTTP`, `Inbound`...

---

## üîß L√†m s·∫°ch d·ªØ li·ªáu ƒë·∫ßu v√†o

Tr∆∞·ªõc khi ƒë∆∞a v√†o m√¥ h√¨nh, c·∫ßn ƒë·∫£m b·∫£o d·ªØ li·ªáu:

* L√†m s·∫°ch t√™n tr∆∞·ªùng d·ªØ li·ªáu
* Ch·ªâ bao g·ªìm c√°c c·ªôt trong `expected_features.txt`
* Kh√¥ng ch·ª©a gi√° tr·ªã `null`, `NaN`, `inf`, `-inf`
* T·∫•t c·∫£ gi√° tr·ªã l√† ki·ªÉu s·ªë (`float` ho·∫∑c `double`)

V√≠ d·ª• ƒëo·∫°n code l√†m s·∫°ch:

```python
from pyspark.sql.functions import col, when

# ‚úÇÔ∏è L√†m s·∫°ch t√™n c·ªôt
df = df.toDF(*[c.strip() for c in df.columns])
for old_name in df.columns:
    new_name = old_name.replace(" ", "_").replace(".", "_")
    if old_name != new_name:
        df = df.withColumnRenamed(old_name, new_name)

# Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng n·∫±m trong expected_features.txt
df = df.select(*expected_features)

# Thay th·∫ø Inf / -Inf b·∫±ng null
for c in df.columns:
    df = df.withColumn(c, when(col(c).isin(float("inf"), float("-inf")), None).otherwise(col(c)))

# Lo·∫°i b·ªè d√≤ng null
df = df.dropna()
```

---

## üß™ Vi·∫øt script ch·∫°y m√¥ h√¨nh real-time (`predict_rf.py`)

B·∫°n c√≥ th·ªÉ vi·∫øt script d·ª± ƒëo√°n theo th·ªùi gian th·ª±c nh∆∞ sau:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, when
from pyspark.sql.types import StructType, DoubleType
from pyspark.ml import PipelineModel

# T·∫°o Spark session v·ªõi Structured Streaming
spark = SparkSession.builder \
    .appName("Real-time DDoS Prediction") \
    .getOrCreate()
spark.sparkContext.setLogLevel("WARN")

# ƒê·ªçc danh s√°ch c·ªôt t·ª´ file expected_features.txt
with open("/opt/ml-model/expected_features.txt") as f:
    feature_cols = [line.strip() for line in f if line.strip()]

# T·∫°o schema cho JSON ƒë·∫ßu v√†o
schema = StructType()
for col_name in feature_cols:
    schema = schema.add(col_name, DoubleType())

# ƒê·ªçc lu·ªìng t·ª´ Kafka
raw_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "ddos_packets_raw") \
    .load()

# Parse JSON v√† ch·ªçn c√°c c·ªôt ƒë√∫ng th·ª© t·ª±
parsed_df = raw_df.selectExpr("CAST(value AS STRING) as json") \
    .select(from_json(col("json"), schema).alias("data")) \
    .select("data.*")

# L√†m s·∫°ch d·ªØ li·ªáu (n·∫øu c·∫ßn)
for c in feature_cols:
    parsed_df = parsed_df.withColumn(c, when(col(c).isin(float("inf"), float("-inf")), None).otherwise(col(c)))
parsed_df = parsed_df.dropna()

# T·∫£i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
model = PipelineModel.load("/opt/ml-model/rf_binary_model")

# D·ª± ƒëo√°n
predictions = model.transform(parsed_df)

# Chuy·ªÉn k·∫øt qu·∫£ ra nh√£n (0.0 ‚Üí Normal, 1.0 ‚Üí DDoS)
result = predictions.withColumn("Label", 
            when(col("prediction") == 1.0, "DDoS").otherwise("Normal")).select("Label")

# Ghi ra console
query = result.writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .start()

query.awaitTermination()
```

---

## üöÄ Ch·∫°y script trong container

```bash
docker exec -it spark-master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-apps/predict_rf.py
```

---

## üí• ƒê·∫ßu ra

D·ª± ƒëo√°n xu·∫•t hi·ªán trong console d∆∞·ªõi d·∫°ng:

```
+----------+
| Label    |
+----------+
| DDoS     |
| Normal   |
+----------+
```

